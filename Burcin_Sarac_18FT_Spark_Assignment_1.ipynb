{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Whitehouse Visitors\n",
    "\n",
    "> Burcin Sarac<br/>\n",
    "> MSc. Business Analytics FT18<br/>\n",
    "> Department of Management Science and Technology<br/>\n",
    "> Athens University of Economics and Business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I used PySpark to answer following questions by using the White House visitors records data released on Obama administration period, gotten from <https://obamawhitehouse.archives.gov/goodgovernment/tools/visitor-records>.\n",
    "\n",
    "1. Who are the top 20 visitors?\n",
    "\n",
    "2. Who are the top 20 visitees?\n",
    "\n",
    "3. Who are the top 20 visitor-visitee pairs?\n",
    "\n",
    "4. What were the top 20 most busy days?\n",
    "\n",
    "5. What where the top 20 most busy months-years?\n",
    "\n",
    "6. What was the order of popularity of days of week for visits? \n",
    "\n",
    "7. What was the order of popularity of months for visits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"E:/dersler/big data systems/Assignment_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running Spark via Python on $Windows$ $10$, there are several steps that needs to be taken care of which are mentioned below; \n",
    "\n",
    "* Downloaded Spark from the [Spark web site](https://spark.apache.org/).\n",
    "\n",
    "* Uncompressed it and placed it into a directory (mine is, `E:/Python/spark/spark2.4.2`).\n",
    "\n",
    "* Added the directory to the path with naming it as `SPARK_HOME`.\n",
    "\n",
    "* Downloaded winutils.exe binary from https://github.com/steveloughran/winutils repository.\n",
    "\n",
    "* Saved winutils.exe binary to a directory of my choice, mine is `c:\\hadoop\\bin`\n",
    "\n",
    "* Again added the directory with setting name as`HADOOP_HOME` (without bin).\n",
    "\n",
    "* Created `C:\\tmp\\hive` directory.\n",
    "\n",
    "* Executed command below from cmd as administrator to give access permission to the spark;\n",
    "`winutils.exe chmod -R 777 C:\\tmp\\hive`\n",
    "\n",
    "* Also installed pyspark and findspark packages via pip from terminal.\n",
    "\n",
    "* And for setting the scene; \n",
    "    * Typed `pyspark` from terminal. \n",
    "    * And created connection and a new SparkContext with codes below to interact with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"E:/Python/spark/spark2.4.2\")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark =  SparkSession.builder.appName(\"Whitehouse_visitors\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The csv files had downloaded as separate files for every year. After uncompressed them, created a new folder called \"visitors\" and added all csv files into it. Then, read the files by executing the command below, which also force the program to read first row as header. With the following commands I tried to view the schema of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = spark.read.\\\n",
    "    option(\"header\", \"true\").\\\n",
    "    option(\"inferSchema\", \"true\").\\\n",
    "    csv(\"visitors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3727273"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+------+------+-----------+-------------+-----+-------------+----+--------------+---------------+--------------+----------------+------------+--------------+----+-------------+---------------+----------------+-----------------+-----------+------------+----------------+-----------------+-----------+--------------------+------------+\n",
      "|NAMELAST|NAMEFIRST|NAMEMID|   UIN|BDGNBR|ACCESS_TYPE|          TOA|  POA|          TOD| POD|APPT_MADE_DATE|APPT_START_DATE| APPT_END_DATE|APPT_CANCEL_DATE|Total_People|LAST_UPDATEDBY|POST|LastEntryDate|TERMINAL_SUFFIX|visitee_namelast|visitee_namefirst|MEETING_LOC|MEETING_ROOM|CALLER_NAME_LAST|CALLER_NAME_FIRST|CALLER_ROOM|         Description|Release Date|\n",
      "+--------+---------+-------+------+------+-----------+-------------+-----+-------------+----+--------------+---------------+--------------+----------------+------------+--------------+----+-------------+---------------+----------------+-----------------+-----------+------------+----------------+-----------------+-----------+--------------------+------------+\n",
      "|    Aaby|   Donett|      L|U83356|     0|         VA|2/26/12 18:50|B0402|         null|null|  2/23/12 0:00|  2/26/12 19:00| 2/26/12 23:59|            null|           8|            KA| WIN|2/23/12 13:32|             KA|            Aaby|             Kate|       OEOB|          8A|            AABY|        KATHERINE|       null|             Bowling|     5/25/12|\n",
      "|    Aaby|    James|      G|U83356|     0|         VA|2/26/12 18:50|B0402|         null|null|  2/23/12 0:00|  2/26/12 19:00| 2/26/12 23:59|            null|           8|            KA| WIN|2/23/12 13:32|             KA|            Aaby|             Kate|       OEOB|          8A|            AABY|        KATHERINE|       null|             Bowling|     5/25/12|\n",
      "|    Aaby|Katherine|      D|U04962|  null|         VA|         null| null|         null|null|   5/7/12 0:00|  5/18/12 11:00| 5/18/12 23:59|            null|           1|            AR| WIN| 5/7/12 15:26|             AR|            Ross|             Adam|       NEOB|       10025|            ROSS|             ADAM|       null|                null|     8/31/12|\n",
      "|    Aaby|Katherine|      D|U08899| 93055|         VA|5/24/12 14:49| K101|5/24/12 16:49|  K1|  5/21/12 0:00|  5/24/12 15:00| 5/24/12 23:59|            null|           1|            AR| WIN|5/21/12 17:42|             AR|            Ross|             Adam|       NEOB|       10025|            ROSS|             ADAM|       null|                null|     8/31/12|\n",
      "| Aadalen|     Kirk|      J|U39090|  null|         VA|         null| null|         null|null|  9/14/12 0:00|  9/18/12 11:30| 9/18/12 23:59|            null|          43|            CM| WIN|  41166.69199|             CM|            null|            POTUS|         WH|  State Floo|  MCNAMARALAWDER|          CLAUDIA|       null|                null|    12/28/12|\n",
      "|  AAFEDT|  ANTHONY|      R|U13475|  null|         VA|         null| null|         null|null|   6/7/12 0:00|    6/8/12 8:00|  6/8/12 23:59|            null|        null|          null|null|         null|           null|            null|             null|       null|        null|            null|             null|       null|                null|        null|\n",
      "|  AAFEDT|     CODY|      A|U13475|  null|         VA|         null| null|         null|null|   6/7/12 0:00|    6/8/12 8:00|  6/8/12 23:59|            null|        null|          null|null|         null|           null|            null|             null|       null|        null|            null|             null|       null|                null|        null|\n",
      "|  AAFEDT|  RAYMOND|      K|U13475|  null|         VA|         null| null|         null|null|   6/7/12 0:00|    6/8/12 8:00|  6/8/12 23:59|            null|        null|          null|null|         null|           null|            null|             null|       null|        null|            null|             null|       null|                null|        null|\n",
      "|  Aafedt|    Dylan|      J|U41863|  null|         VA|         null| null|         null|null|  9/25/12 0:00|  10/5/12 12:30| 10/5/12 23:59|            null|         268|            EY| WIN|9/25/12 13:33|             EY|          OFFICE|         VISITORS|         WH|   RESIDENCE|          OFFICE|         VISITORS|       null|          GROUP TOUR|     1/25/13|\n",
      "|  Aafedt|  Jeffrey|      A|U41863|  null|         VA|         null| null|         null|null|  9/25/12 0:00|  10/5/12 12:30| 10/5/12 23:59|            null|         268|            EY| WIN|9/25/12 13:33|             EY|          OFFICE|         VISITORS|         WH|   RESIDENCE|          OFFICE|         VISITORS|       null|          GROUP TOUR|     1/25/13|\n",
      "|  Aafedt|    Robyn|      K|U41863|  null|         VA|         null| null|         null|null|  9/25/12 0:00|  10/5/12 12:30| 10/5/12 23:59|            null|         268|            EY| WIN|9/25/12 13:33|             EY|          OFFICE|         VISITORS|         WH|   RESIDENCE|          OFFICE|         VISITORS|       null|          GROUP TOUR|     1/25/13|\n",
      "| Aagaard| Kimberly|   null|U59552|  null|         VA|         null| null|         null|null|  12/6/12 0:00| 12/15/12 20:30|12/15/12 23:59|            null|         254|            CB| WIN|12/6/12 12:08|             CB|          OFFICE|         VISITORS|         WH|   RESIDENCE|          OFFICE|         VISITORS|       null|          GROUP TOUR|     3/29/13|\n",
      "| AAGENES|     ANNA|   null|  null|  null|       null|         null| null|         null|null|          null|        9/19/12|          null|            null|        null|          null|null|         null|           null|            null|             null|        VPR|        null|            null|             null|       null|Reception with LG...|    12/28/12|\n",
      "| Aagenes|     Anna|      L|U38391|  null|         VA|         null| null|         null|null|  9/17/12 0:00|   9/19/12 8:30| 9/19/12 23:59|            null|         291|            CP| WIN|9/17/12 16:20|             CP|          OFFICE|         VISITORS|         WH|   RESIDENCE|          OFFICE|         VISITORS|       null|          GROUP TOUR|    12/28/12|\n",
      "| Aagenes|     Anna|      L|U39917| 90174|         VA| 9/19/12 9:42|D0101|9/19/12 12:56|  D4|  9/18/12 0:00|  9/19/12 10:00| 9/19/12 23:59|            null|         158|            VM| WIN|9/18/12 15:04|             VM|        RAGHAVAN|           GAUTAM|       OEOB|  SOUTH COUR|      MCCULLOUGH|         VICTORIA|       null|TIME CHANGE AT TH...|    12/28/12|\n",
      "|  Aakhus|    Blake|      n|U91790|  null|         VA|         null| null|         null|null|  3/22/12 0:00|   4/5/12 10:30|  4/5/12 23:59|            null|         273|            EH| WIN|3/22/12 12:26|             EH|          OFFICE|         VISITORS|         WH|   RESIDENCE|          OFFICE|         VISITORS|       null|          GROUP TOUR|     7/27/12|\n",
      "|  Aakhus|   Carter|      E|U91790|  null|         VA|         null| null|         null|null|  3/23/12 0:00|   4/5/12 10:30|  4/5/12 23:59|            null|         273|            EH| WIN| 3/23/12 7:34|             EH|          OFFICE|         VISITORS|         WH|   RESIDENCE|          OFFICE|         VISITORS|       null|          GROUP TOUR|     7/27/12|\n",
      "|  Aakhus|  Gregory|      D|U91790|  null|         VA|         null| null|         null|null|  3/23/12 0:00|   4/5/12 10:30|  4/5/12 23:59|            null|         273|            EH| WIN| 3/23/12 7:34|             EH|          OFFICE|         VISITORS|         WH|   RESIDENCE|          OFFICE|         VISITORS|       null|          GROUP TOUR|     7/27/12|\n",
      "|  Aakhus|    Kelly|      V|U91790|  null|         VA|         null| null|         null|null|  3/23/12 0:00|   4/5/12 10:30|  4/5/12 23:59|            null|         273|            EH| WIN| 3/23/12 7:34|             EH|          OFFICE|         VISITORS|         WH|   RESIDENCE|          OFFICE|         VISITORS|       null|          GROUP TOUR|     7/27/12|\n",
      "|  AalAli|   Ameena|      K|U92835|  null|         VA|         null| null|         null|null|  3/27/12 0:00|   4/3/12 10:30|  4/3/12 23:59|            null|         277|            TH| WIN| 3/27/12 8:09|             TH|          OFFICE|         VISITORS|         WH|   RESIDENCE|          OFFICE|         VISITORS|       null|          GROUP TOUR|     7/27/12|\n",
      "+--------+---------+-------+------+------+-----------+-------------+-----+-------------+----+--------------+---------------+--------------+----------------+------------+--------------+----+-------------+---------------+----------------+-----------------+-----------+------------+----------------+-----------------+-----------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NAMELAST: string (nullable = true)\n",
      " |-- NAMEFIRST: string (nullable = true)\n",
      " |-- NAMEMID: string (nullable = true)\n",
      " |-- UIN: string (nullable = true)\n",
      " |-- BDGNBR: string (nullable = true)\n",
      " |-- ACCESS_TYPE: string (nullable = true)\n",
      " |-- TOA: string (nullable = true)\n",
      " |-- POA: string (nullable = true)\n",
      " |-- TOD: string (nullable = true)\n",
      " |-- POD: string (nullable = true)\n",
      " |-- APPT_MADE_DATE: string (nullable = true)\n",
      " |-- APPT_START_DATE: string (nullable = true)\n",
      " |-- APPT_END_DATE: string (nullable = true)\n",
      " |-- APPT_CANCEL_DATE: string (nullable = true)\n",
      " |-- Total_People: string (nullable = true)\n",
      " |-- LAST_UPDATEDBY: string (nullable = true)\n",
      " |-- POST: string (nullable = true)\n",
      " |-- LastEntryDate: string (nullable = true)\n",
      " |-- TERMINAL_SUFFIX: string (nullable = true)\n",
      " |-- visitee_namelast: string (nullable = true)\n",
      " |-- visitee_namefirst: string (nullable = true)\n",
      " |-- MEETING_LOC: string (nullable = true)\n",
      " |-- MEETING_ROOM: string (nullable = true)\n",
      " |-- CALLER_NAME_LAST: string (nullable = true)\n",
      " |-- CALLER_NAME_FIRST: string (nullable = true)\n",
      " |-- CALLER_ROOM: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Release Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since dataframe includes different data types, I created a schema first according to provided data explanation and read the dataset again this time by using schema command, however it did convert all observations into null data, so I decided not to follow that path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark needs to re-read and parse again the data everytime we asked for an action.\n",
    "\n",
    "Since I will be using the data again and again, it makes sense to cache the parse results, so that they are already available and I do not need to re-read and re-parse them.\n",
    "\n",
    "By using `cache()`, Spark will try to persist the data in memory in each partition node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[NAMELAST: string, NAMEFIRST: string, NAMEMID: string, UIN: string, BDGNBR: string, ACCESS_TYPE: string, TOA: string, POA: string, TOD: string, POD: string, APPT_MADE_DATE: string, APPT_START_DATE: string, APPT_END_DATE: string, APPT_CANCEL_DATE: string, Total_People: string, LAST_UPDATEDBY: string, POST: string, LastEntryDate: string, TERMINAL_SUFFIX: string, visitee_namelast: string, visitee_namefirst: string, MEETING_LOC: string, MEETING_ROOM: string, CALLER_NAME_LAST: string, CALLER_NAME_FIRST: string, CALLER_ROOM: string, Description: string, Release Date: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|APPT_CANCEL_DATE|count|\n",
      "+----------------+-----+\n",
      "|   4/23/12 15:41|   40|\n",
      "|   9/26/12 15:10|    1|\n",
      "|    1/30/12 9:45|    1|\n",
      "|10/26/2010 10:41|    2|\n",
      "|11/15/2010 18:11|    1|\n",
      "|   2/1/2011 9:05|    5|\n",
      "|  2/3/2011 17:35|    1|\n",
      "| 4/14/2010 10:07|    1|\n",
      "|        3/7/2010|    3|\n",
      "|  6/8/2011 10:54|    1|\n",
      "|  10/11/11 11:31|    1|\n",
      "|11/21/2011 10:11|    1|\n",
      "|10/15/2013 16:04|    1|\n",
      "| 3/27/2013 13:03|    1|\n",
      "| 1/15/2014 10:47|    1|\n",
      "|     41675.74465|    1|\n",
      "|   3/19/12 17:28|    1|\n",
      "| 10/8/2010 17:11|    1|\n",
      "| 1/18/2011 14:13|    3|\n",
      "| 7/28/2010 12:30|   45|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.filter(parsed.APPT_CANCEL_DATE.isNotNull()).groupBy(\"APPT_CANCEL_DATE\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the command above I checked if were there any cancelled appointments and it seems there were. I used this information at the following questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-1\n",
    "\n",
    "1. Who are the top 20 visitors?\n",
    "\n",
    "For solving this question, first I filtered observations who cancelled their appointment by using `APPT_CANCEL_DATE` column data. Then I groupped by data into `NAMELAST` and `NAMEFIRST` columns and count. With `orderBy` command, I ordered data and it shows first 20 highest counted rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----+\n",
      "|    NAMELAST|NAMEFIRST|count|\n",
      "+------------+---------+-----+\n",
      "|        Hash|  Michael|  726|\n",
      "|    Tavenner|  Marilyn|  519|\n",
      "|        Hoff|    James|  482|\n",
      "|BrooksLaSure| Chiquita|  410|\n",
      "|     Levitis|    Jason|  384|\n",
      "|       Borzi|  Phyllis|  368|\n",
      "|    Fontenot|   Yvette|  350|\n",
      "|      Khalid|   Aryana|  341|\n",
      "|        Mann|  Cynthia|  323|\n",
      "|      Werner|   Sharon|  310|\n",
      "|        Choe|  Kenneth|  305|\n",
      "|       Smith|  Michael|  304|\n",
      "|       Jones|   Daniel|  303|\n",
      "|      Turner|      Amy|  302|\n",
      "|    Morrison|    Helen|  295|\n",
      "|  Livingston|Catherine|  295|\n",
      "|       Lewis|     Caya|  292|\n",
      "|     Kronick|  Richard|  292|\n",
      "|     Maguire|   Daniel|  290|\n",
      "|     Schultz|  William|  283|\n",
      "+------------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.\\\n",
    "    filter(parsed.APPT_CANCEL_DATE.isNull()).\\\n",
    "    groupBy(\"NAMELAST\", \"NAMEFIRST\").\\\n",
    "    count().\\\n",
    "    orderBy(\"count\", ascending=False).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-2\n",
    "\n",
    "2. Who are the top 20 visitees?\n",
    "\n",
    "For this question, again I began with filtered observations who cancelled their appointment by using `APPT_CANCEL_DATE` column data, additionally I also filter observations under `visitee_namelast` and `visitee_namefirst` data, which did not refer to a visitee name. I also dropped null data by using `na.drop` command. Then I groupped by data into  `visitee_namelast` and `visitee_namefirst` columns and count. With `orderBy` command, I ordered data and it shows first 20 highest counted visitee name and surname combinations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+-----+\n",
      "|visitee_namelast|visitee_namefirst|count|\n",
      "+----------------+-----------------+-----+\n",
      "|         Lierman|             Kyle|14453|\n",
      "|         Lambrew|           Jeanne|14127|\n",
      "|         DOEBLER|              MAX| 5962|\n",
      "|      MCCULLOUGH|         VICTORIA| 5720|\n",
      "|        Sperling|             Gene| 4327|\n",
      "|          NELSON|             GREG| 4278|\n",
      "|           Hogan|            Nancy| 4226|\n",
      "|          DuBois|           Joshua| 4198|\n",
      "|        Raghavan|           Gautam| 3985|\n",
      "|          Foster|          Heather| 3900|\n",
      "|        Monteiro|             Paul| 3885|\n",
      "|            Park|             Todd| 3657|\n",
      "|      Richardson|            Karen| 3546|\n",
      "|         McKalip|             Doug| 3338|\n",
      "|         doebler|              max| 3259|\n",
      "|           Kelly|           Lauren| 3216|\n",
      "|             Cho|           Ronnie| 3189|\n",
      "|            FENN|            SARAH| 3149|\n",
      "|           kelly|           lauren| 3120|\n",
      "|        monteiro|             paul| 3092|\n",
      "+----------------+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.\\\n",
    "    filter((parsed.visitee_namelast != \"OFFICE\")&(parsed.visitee_namelast != \"/\")&(parsed.visitee_namefirst != \"OFFICE\")&\n",
    "          (parsed.visitee_namelast != \"and\")).\\\n",
    "    filter(parsed.APPT_CANCEL_DATE.isNull()).\\\n",
    "    groupBy(\"visitee_namelast\", \"visitee_namefirst\").\\\n",
    "    count().\\\n",
    "    orderBy(\"count\", ascending=False).\\\n",
    "    na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-3\n",
    "\n",
    "3. Who are the top 20 visitor-visitee pairs?\n",
    "\n",
    "For this question, again I began with filtered observations who cancelled their appointment by using APPT_CANCEL_DATE column data, additionally I also filter observations under visitee_namelast and visitee_namefirst data, which did not refer to a visitee name. I also dropped null data by using na.drop command. But this time, I groupped by data into visitee_namelast and visitee_namefirst columns together with `NAMELAST` and `NAMEFIRST` columns and count. With orderBy command, I ordered data and it shows first 20 highest counted visitee - visitor pairs with name and surname combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+------------+---------+-----+\n",
      "|visitee_namelast|visitee_namefirst|    NAMELAST|NAMEFIRST|count|\n",
      "+----------------+-----------------+------------+---------+-----+\n",
      "|         Lambrew|           Jeanne|        Hash|  Michael|  515|\n",
      "|            Hoff|           Joanne|        Hoff|    James|  387|\n",
      "|         Lambrew|           Jeanne|    Tavenner|  Marilyn|  379|\n",
      "|         Lambrew|           Jeanne|BrooksLaSure| Chiquita|  324|\n",
      "|         Lambrew|           Jeanne|    Fontenot|   Yvette|  292|\n",
      "|         Lambrew|           Jeanne|      Khalid|   Aryana|  283|\n",
      "|         Lambrew|           Jeanne|     Levitis|    Jason|  280|\n",
      "|         Lambrew|           Jeanne|        Mann|  Cynthia|  269|\n",
      "|         Lambrew|           Jeanne|     Kronick|  Richard|  252|\n",
      "|         Lambrew|           Jeanne|        Choe|  Kenneth|  242|\n",
      "|         Lambrew|           Jeanne|  Livingston|Catherine|  240|\n",
      "|         Lambrew|           Jeanne|    Morrison|    Helen|  235|\n",
      "|         Lambrew|           Jeanne|       Borzi|  Phyllis|  233|\n",
      "|         Lambrew|           Jeanne|     Maguire|   Daniel|  229|\n",
      "|         Lambrew|           Jeanne|      Turner|      Amy|  228|\n",
      "|         Lambrew|           Jeanne|      Larsen|   Steven|  212|\n",
      "|         Lambrew|           Jeanne|      Obrien|     John|  204|\n",
      "|         Lambrew|           Jeanne|       Davis| Jonathan|  200|\n",
      "|         Lambrew|           Jeanne|       Foley| Jonathan|  197|\n",
      "|         Lambrew|           Jeanne|     Elliott|   Nicole|  194|\n",
      "+----------------+-----------------+------------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.\\\n",
    "    filter((parsed.visitee_namelast != \"OFFICE\")&(parsed.visitee_namelast != \"/\")&(parsed.visitee_namefirst != \"OFFICE\")).\\\n",
    "    filter(parsed.APPT_CANCEL_DATE.isNull()).\\\n",
    "    groupBy(\"visitee_namelast\",\"visitee_namefirst\", \"NAMELAST\", \"NAMEFIRST\").\\\n",
    "    count().\\\n",
    "    orderBy(\"count\", ascending=False).\\\n",
    "    na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-4\n",
    "\n",
    "4. What were the top 20 most busy days?\n",
    "\n",
    "This time, for the following questions, I was needed to convert `APPT_MADE_DATE` column into timestamp datatype. Since the dataset was not clean and one typed organized, fitting a previously created Schema when reading csv files did not performed as expected and produced null values for all columns. So I decided to convert only the required date column and separate them from the whole dataframe for answering following questions. \n",
    "\n",
    "So I first imported `unix_timestamp`and `from_unixtime`functions from `pyspark.sql.functions` library. With the helps of these funtions, I created a new dataframe only includes `APPT_MADE_DATE` column and its converted version named `appt_m_date` by using following commands. \n",
    "\n",
    "After converting related column, I first checked if it can read the timestamp data as expected by checking days of week. And then for answering the question, I let the command count the days by appearance from converted `appt_m_date` column and order them from highest to lowest. And I also dropped null data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+-------------------+\n",
      "|APPT_MADE_DATE|Total_People|        appt_m_date|\n",
      "+--------------+------------+-------------------+\n",
      "|  2/23/12 0:00|           8|2012-02-23 00:00:00|\n",
      "|  2/23/12 0:00|           8|2012-02-23 00:00:00|\n",
      "|   5/7/12 0:00|           1|2012-05-07 00:00:00|\n",
      "|  5/21/12 0:00|           1|2012-05-21 00:00:00|\n",
      "|  9/14/12 0:00|          43|2012-09-14 00:00:00|\n",
      "|   6/7/12 0:00|        null|2012-06-07 00:00:00|\n",
      "|   6/7/12 0:00|        null|2012-06-07 00:00:00|\n",
      "|   6/7/12 0:00|        null|2012-06-07 00:00:00|\n",
      "|  9/25/12 0:00|         268|2012-09-25 00:00:00|\n",
      "|  9/25/12 0:00|         268|2012-09-25 00:00:00|\n",
      "|  9/25/12 0:00|         268|2012-09-25 00:00:00|\n",
      "|  12/6/12 0:00|         254|2012-12-06 00:00:00|\n",
      "|          null|        null|               null|\n",
      "|  9/17/12 0:00|         291|2012-09-17 00:00:00|\n",
      "|  9/18/12 0:00|         158|2012-09-18 00:00:00|\n",
      "|  3/22/12 0:00|         273|2012-03-22 00:00:00|\n",
      "|  3/23/12 0:00|         273|2012-03-23 00:00:00|\n",
      "|  3/23/12 0:00|         273|2012-03-23 00:00:00|\n",
      "|  3/23/12 0:00|         273|2012-03-23 00:00:00|\n",
      "|  3/27/12 0:00|         277|2012-03-27 00:00:00|\n",
      "+--------------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "parsed2 = parsed.select('APPT_MADE_DATE','Total_People', \n",
    "                        from_unixtime(unix_timestamp('APPT_MADE_DATE', 'MM/dd/yy')).alias('appt_m_date'))\n",
    "\n",
    "parsed2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "\n",
    "prepped_dataframe = parsed2\\\n",
    "  .na.fill(0)\\\n",
    "  .withColumn(\"day_of_week\", date_format(col(\"appt_m_date\"), \"EEEE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|day_of_week|\n",
      "+-----------+\n",
      "|       null|\n",
      "|     Friday|\n",
      "|     Monday|\n",
      "|   Saturday|\n",
      "|     Sunday|\n",
      "|   Thursday|\n",
      "|    Tuesday|\n",
      "|  Wednesday|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepped_dataframe.select(\"day_of_week\").distinct().orderBy(\"day_of_week\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|        appt_m_date|count|\n",
      "+-------------------+-----+\n",
      "|2012-05-08 00:00:00|12301|\n",
      "|2011-12-06 00:00:00|11572|\n",
      "|2011-12-07 00:00:00|11335|\n",
      "|2010-12-15 00:00:00|10556|\n",
      "|2012-03-19 00:00:00|10312|\n",
      "|2012-12-05 00:00:00|10059|\n",
      "|2009-12-08 00:00:00| 9448|\n",
      "|2009-12-09 00:00:00| 9389|\n",
      "|2011-10-27 00:00:00| 9379|\n",
      "|2012-06-21 00:00:00| 9341|\n",
      "|2013-11-22 00:00:00| 9271|\n",
      "|2012-12-03 00:00:00| 9252|\n",
      "|2012-03-20 00:00:00| 9143|\n",
      "|2013-02-20 00:00:00| 9127|\n",
      "|2011-10-12 00:00:00| 8990|\n",
      "|2012-12-06 00:00:00| 8979|\n",
      "|2012-04-11 00:00:00| 8920|\n",
      "|2012-03-13 00:00:00| 8837|\n",
      "|2013-03-07 00:00:00| 8730|\n",
      "|2012-07-25 00:00:00| 8555|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed2.\\\n",
    "    groupBy(\"appt_m_date\").\\\n",
    "    count().\\\n",
    "    orderBy(\"count\", ascending=False).\\\n",
    "    na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-5\n",
    "\n",
    "5. What where the top 20 most busy months-years?\n",
    "\n",
    "This time,I again used same converted data frame but this time I groupped by data by Month and Year pairs, again order them by appearance and dropped the null data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_dataframe2 = parsed2\\\n",
    "  .na.fill(0)\\\n",
    "  .withColumn(\"by_month_year\", date_format(col(\"appt_m_date\"), \"MMM-YYY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|by_month_year|\n",
      "+-------------+\n",
      "|         null|\n",
      "|     Apr-2009|\n",
      "|     Apr-2010|\n",
      "|     Apr-2011|\n",
      "|     Apr-2012|\n",
      "|     Apr-2013|\n",
      "|     Aug-2009|\n",
      "|     Aug-2010|\n",
      "|     Aug-2011|\n",
      "|     Aug-2012|\n",
      "|     Aug-2013|\n",
      "|     Dec-2008|\n",
      "|     Dec-2009|\n",
      "|     Dec-2010|\n",
      "|     Dec-2011|\n",
      "|     Dec-2012|\n",
      "|     Dec-2013|\n",
      "|     Dec-2014|\n",
      "|     Feb-2009|\n",
      "|     Feb-2010|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepped_dataframe2.select(\"by_month_year\").distinct().orderBy(\"by_month_year\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|by_month_year| count|\n",
      "+-------------+------+\n",
      "|     Dec-2009|112023|\n",
      "|     Jun-2010|109463|\n",
      "|     Jul-2012|104254|\n",
      "|     Mar-2010|103741|\n",
      "|     Jun-2012| 98570|\n",
      "|     Oct-2011| 98237|\n",
      "|     Jun-2011| 97600|\n",
      "|     Dec-2010| 96908|\n",
      "|     May-2011| 95355|\n",
      "|     Mar-2012| 91679|\n",
      "|     Mar-2011| 91468|\n",
      "|     Jul-2011| 89092|\n",
      "|     Dec-2011| 88418|\n",
      "|     May-2010| 88302|\n",
      "|     May-2012| 87271|\n",
      "|     Aug-2011| 83919|\n",
      "|     Dec-2012| 83144|\n",
      "|     Apr-2012| 82220|\n",
      "|     Apr-2011| 81539|\n",
      "|     Aug-2010| 79755|\n",
      "+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepped_dataframe2.\\\n",
    "    groupBy(\"by_month_year\").\\\n",
    "    count().\\\n",
    "    orderBy(\"count\", ascending=False).\\\n",
    "    na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-6\n",
    "\n",
    "6. What was the order of popularity of days of week for visits? \n",
    "\n",
    "This time,I again used same converted data frame and same dataframe which I created for the Q4 with the commands above. Then I groupped by data by Days, again order them by appearance and dropped the null data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|day_of_week| count|\n",
      "+-----------+------+\n",
      "|    Tuesday|776124|\n",
      "|  Wednesday|730959|\n",
      "|   Thursday|701142|\n",
      "|     Monday|647961|\n",
      "|     Friday|647596|\n",
      "|   Saturday| 99687|\n",
      "|     Sunday| 40514|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepped_dataframe.\\\n",
    "    groupBy(\"day_of_week\").\\\n",
    "    count().\\\n",
    "    orderBy(\"count\", ascending=False).\\\n",
    "    na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-7\n",
    "\n",
    "7. What was the order of popularity of months for visits?\n",
    "\n",
    "This time,I again used same converted data frame and follow same path as I used at the previous question, only difference was pointing out months instead of days this time. Then I groupped by data by Months, again order them by appearance and dropped the null data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_dataframe3 = parsed2\\\n",
    "  .na.fill(0)\\\n",
    "  .withColumn(\"by_month\", date_format(col(\"appt_m_date\"), \"MMM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|by_month| count|\n",
      "+--------+------+\n",
      "|     Dec|457258|\n",
      "|     Mar|385443|\n",
      "|     Nov|340281|\n",
      "|     Oct|339934|\n",
      "|     Jun|325137|\n",
      "|     May|292951|\n",
      "|     Jul|270481|\n",
      "|     Apr|266545|\n",
      "|     Feb|254596|\n",
      "|     Sep|251530|\n",
      "|     Aug|243136|\n",
      "|     Jan|216691|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepped_dataframe3.\\\n",
    "    groupBy(\"by_month\").\\\n",
    "    count().\\\n",
    "    orderBy(\"count\", ascending=False).\\\n",
    "    na.drop().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
